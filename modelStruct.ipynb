{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # requires: pip install pandas\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(4096, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import T5ForConditionalGeneration\n",
    "# import copy\n",
    "\n",
    "# class CustomT5Block(nn.Module):\n",
    "#     def __init__(self, original_block):\n",
    "#         super().__init__()\n",
    "#         self.original_block = original_block\n",
    "\n",
    "#         # Define the new feedforward layer\n",
    "#         self.additional_ff = nn.Sequential(\n",
    "#             nn.Linear(512, 512),  # Input 512, output 512\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x, **kwargs):\n",
    "#         # Pass through the original block\n",
    "#         x, *additional_outputs = self.original_block(x, **kwargs)\n",
    "        \n",
    "#         # Check if x is a tensor and not a tuple\n",
    "#         if isinstance(x, tuple):\n",
    "#             x = x[0]  # Get the first tensor if it's a tuple\n",
    "\n",
    "#         # Pass through the additional feedforward layer\n",
    "#         x = self.additional_ff(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class ModifiedT5(nn.Module):\n",
    "#     def __init__(self, original_model):\n",
    "#         super(ModifiedT5, self).__init__()\n",
    "#         self.shared = original_model.shared\n",
    "#         self.encoder = original_model.encoder\n",
    "#         self.decoder = original_model.decoder\n",
    "#         self.lm_head = original_model.lm_head\n",
    "        \n",
    "#         # Create new T5Block instances with the additional FF\n",
    "#         self.encoder.block = nn.ModuleList(\n",
    "#             CustomT5Block(block) for block in original_model.encoder.block\n",
    "#         )\n",
    "        \n",
    "#         self.decoder.block = nn.ModuleList(\n",
    "#             CustomT5Block(block) for block in original_model.decoder.block\n",
    "#         )\n",
    "\n",
    "#         # Define additional layers\n",
    "#         self.additional_dense = nn.Linear(512, 512)\n",
    "#         self.layer_norm = nn.LayerNorm(512)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask=None, decoder_input_ids=None):\n",
    "#         # Pass input through the encoder\n",
    "#         encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "#         # Prepare decoder input IDs if not provided\n",
    "#         if decoder_input_ids is None:\n",
    "#             decoder_input_ids = input_ids  # Or use a different approach as per your task\n",
    "        \n",
    "#         # Pass the encoder outputs to the decoder\n",
    "#         decoder_outputs = self.decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_outputs[0])\n",
    "        \n",
    "#         # Pass decoder outputs through the language model head\n",
    "#         lm_logits = self.lm_head(decoder_outputs[0])\n",
    "        \n",
    "#         # Pass through additional dense layer, layer norm, and dropout\n",
    "#         lm_logits = self.additional_dense(lm_logits)\n",
    "#         lm_logits = self.layer_norm(lm_logits)\n",
    "#         lm_logits = self.dropout(lm_logits)\n",
    "        \n",
    "#         return lm_logits\n",
    "\n",
    "# # Usage\n",
    "# original_model = T5ForConditionalGeneration.from_pretrained(\"amazon/chronos-t5-small\")\n",
    "# copied_model = copy.deepcopy(original_model)\n",
    "# modified_model = ModifiedT5(copied_model)\n",
    "\n",
    "# # # Test with dummy input\n",
    "# # input_ids = torch.randint(0, 4096, (1, 10))  # Batch size 1, sequence length 10\n",
    "# # output = modified_model(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['encoder.block.0.layer.2.additional_ff.0.weight', 'encoder.block.0.layer.2.additional_ff.0.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import set_seed\n",
    "import torch.nn as nn\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class CustomFFLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the new feedforward layer\n",
    "        self.additional_ff = nn.Sequential(\n",
    "            nn.Linear(512, 512),  # Input 512, output 400\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, **kwargs):        \n",
    "        # Pass through the additional feedforward layer\n",
    "        x = self.additional_ff(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Access the first encoder block\n",
    "        first_encoder_block = self.encoder.block[0]\n",
    "        customLayer = CustomFFLayer()\n",
    "        # Duplicate the T5LayerFF layer (which is the second layer in the block)\n",
    "        # extra_ff_layer = copy.deepcopy(first_encoder_block.layer[1])  # The FF layer is the second layer in the block\n",
    "\n",
    "        first_encoder_block.layer.append(customLayer)  # Add it again\n",
    "    \n",
    "        # Do the same for the first decoder block\n",
    "        # first_decoder_block = self.decoder.block[0]\n",
    "        # extra_ff_layer_decoder = copy.deepcopy(first_decoder_block.layer[2])  # The FF layer is the third layer in the decoder block\n",
    "        # first_decoder_block.layer.append(extra_ff_layer_decoder)\n",
    "\n",
    "# Load the pre-trained model\n",
    "original_model = T5ForConditionalGeneration.from_pretrained(\"amazon/chronos-t5-small\")\n",
    "\n",
    "copy_model = copy.deepcopy(original_model)\n",
    "\n",
    "# Initialize custom model with the same configuration\n",
    "custom_model = CustomT5(copy_model.config)\n",
    "\n",
    "# Load the weights from the pre-trained model into the custom model\n",
    "custom_model.load_state_dict(copy_model.state_dict(), strict=False)  # strict=False allows for architectural changes\n",
    "\n",
    "# Now custom_model should share weights with the original model, except for the new FF layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomT5(\n",
       "  (shared): Embedding(4096, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): CustomFFLayer(\n",
       "            (additional_ff): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=400, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.chronos.chronos\n",
    "import importlib\n",
    "importlib.reload(src.chronos.chronos)\n",
    "\n",
    "from src.chronos.chronos import ChronosPipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "context = torch.linspace(0, 100, 512).unsqueeze(0)\n",
    "\n",
    "\n",
    "token_ids, attention_mask, scale = (\n",
    "    pipeline.tokenizer.context_input_transform(context)\n",
    ")\n",
    "\n",
    "# import pprint\n",
    "# pprint.pprint(output)\n",
    "token_ids = token_ids[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7c48998b90>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_from_original_model = []\n",
    "remaining = 50\n",
    "\n",
    "while remaining > 0:\n",
    "    token_ids, attention_mask, scale = pipeline.tokenizer.context_input_transform(\n",
    "        context\n",
    "    )\n",
    "    samples = original_model.generate(\n",
    "                input_ids=token_ids,\n",
    "                attention_mask=attention_mask, \n",
    "                min_new_tokens=50,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=pipeline.model.config.eos_token_id,\n",
    "                pad_token_id=pipeline.model.config.pad_token_id)\n",
    "    \n",
    "    prediction = pipeline.tokenizer.output_transform(\n",
    "        samples.to(scale.device), scale\n",
    "    )\n",
    "\n",
    "    predictions_from_original_model.append(prediction)\n",
    "    remaining -= prediction.shape[-1]\n",
    "\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    context = torch.cat(\n",
    "        [context, prediction.median(dim=1).values], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "data_custom_to_plot = torch.concat((context,predictions_from_original_model[0][0,:,1:]),dim=1).squeeze(0).tolist()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data_custom_to_plot = np.array(data_custom_to_plot)\n",
    "plt.plot(data_custom_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (400) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     token_ids, attention_mask, scale \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcontext_input_transform(\n\u001b[1;32m      6\u001b[0m         context\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39moutput_transform(\n\u001b[1;32m     19\u001b[0m         samples\u001b[38;5;241m.\u001b[39mto(scale\u001b[38;5;241m.\u001b[39mdevice), scale\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     predictions_from_custom_model\u001b[38;5;241m.\u001b[39mappend(prediction)\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/generation/utils.py:1745\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   1740\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor, generation_config\u001b[38;5;241m.\u001b[39m_eos_token_tensor\n\u001b[1;32m   1741\u001b[0m     )\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/generation/utils.py:549\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    548\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 549\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1106\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1092\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1093\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         output_attentions,\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:686\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    696\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:592\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[0;32m--> 592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    594\u001b[0m         normed_hidden_states,\n\u001b[1;32m    595\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DATA/Conda_Envs/procGen/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:252\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16]:\n\u001b[1;32m    250\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (400) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "predictions_from_custom_model = []\n",
    "remaining = 50\n",
    "\n",
    "while remaining > 0:\n",
    "    token_ids, attention_mask, scale = pipeline.tokenizer.context_input_transform(\n",
    "        context\n",
    "    )\n",
    "    samples = custom_model.generate(\n",
    "                input_ids=token_ids,\n",
    "                attention_mask=attention_mask, \n",
    "                min_new_tokens=50,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=pipeline.model.config.eos_token_id,\n",
    "                pad_token_id=pipeline.model.config.pad_token_id)\n",
    "    \n",
    "    prediction = pipeline.tokenizer.output_transform(\n",
    "        samples.to(scale.device), scale\n",
    "    )\n",
    "\n",
    "    predictions_from_custom_model.append(prediction)\n",
    "    remaining -= prediction.shape[-1]\n",
    "\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    context = torch.cat(\n",
    "        [context, prediction.median(dim=1).values], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "data_custom_to_plot = torch.concat((context,predictions_from_custom_model[0][0,:,1:]),dim=1).squeeze(0).tolist()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data_custom_to_plot = np.array(data_custom_to_plot)\n",
    "plt.plot(data_custom_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pipeline.predict(context=context, prediction_length=50, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000,   0.0000,  -0.3666,   0.0000,   0.0000,   0.3666,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.3666,\n",
       "            0.0000,  -0.3666,   0.0000, 103.7390,   0.0000,  -0.3666,  -0.3666,\n",
       "            0.0000,  -0.3666,   0.0000,   0.0000,   0.0000,   0.0000,  -0.3666,\n",
       "            0.0000,  -0.3666,   0.0000,   0.0000,   0.0000,  -0.3666,  -0.3666,\n",
       "            0.0000,  -0.3666,   0.0000,  -0.3666,   0.3666,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,  -0.3666,   0.0000,   0.0000,   0.0000,  -0.3666,\n",
       "            0.0000]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast - predictions_from_original_model[0][:,:,1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
